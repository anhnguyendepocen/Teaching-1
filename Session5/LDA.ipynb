{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "In this course we will delve into Dirichlet Allocation which is more powerfull model probabilistic Latent Semantic Analysis (pLSA) that we covered last time. \n",
    "\n",
    "![The generative story of LDA](./my_lda.png)\n",
    "\n",
    "The model is given text and given the assumptions of the generative story as described by the above figure it returns the per-topic word distributions (left figure) and the per-document topic distributions. \n",
    "\n",
    "In this session our goal is to code LDA. The model should be called with a number of topics and the values of the hyper-parameters $\\alpha$ and $\\beta$ as arguments. Below, you can find the equations and the structure of a class `LDA` that you are required to populate.\n",
    "\n",
    "\n",
    "## Pseudocode for Gibbs Sampling updates\n",
    "\n",
    "![Pseudocode for Gibbs Sampling LDA](./lda_pseudocode.png)\n",
    "\n",
    "Note that the counts stand for: \n",
    "\n",
    "* $n_{d,k}$ : # of times topic $k$ is assigned to document $d$\n",
    "* $n_{k,w}$ : # of times topic $k$ is assigned to word $w$ \n",
    "* $n_k$ : # of words assigned to topic $k$ \n",
    "* $z_{m,n}$ : topic assignments of each of the $n$ words for the $m$ documents\n",
    "\n",
    "The pseudocode is copied from http://u.cs.biu.ac.il/~89-680/darling-lda.pdf.\n",
    "\n",
    "\n",
    "## Todos: \n",
    "1. Load data `wikipedia_data.txt`\n",
    "2. Generate the correct representation to be fed in LDA (as described in the \\__init\\__() of LDA_Gibbs_Sampling below\n",
    "3. Train LDA for several iterations\n",
    "4. Visualize top words for a topic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2825\n"
     ]
    }
   ],
   "source": [
    "documents = open('wikipedia_data.txt').read().splitlines()\n",
    "print(len(documents))\n",
    "vocabulary = {}\n",
    "stopwords = set( open('stop_words.txt').read().splitlines()) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_lda_format = []\n",
    "\n",
    "for document in documents:\n",
    "    document_lda_format = []\n",
    "    words = document.split()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        \n",
    "        if word.isalpha() and word not in stopwords:\n",
    "            try:\n",
    "                document_lda_format.append(vocabulary[word])\n",
    "            except:\n",
    "                vocabulary[word] = len(vocabulary)\n",
    "                document_lda_format.append(vocabulary[word])\n",
    "    documents_lda_format.append(document_lda_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2825, 2825)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents), len(documents_lda_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class lda_gibbs_sampling:\n",
    "    def __init__(self, K=25, alpha=0.5, beta=0.5, docs= None, V= None):\n",
    "        \"\"\"Initialization function.\n",
    "        K : number of topics, integer\n",
    "        alpha: Dirichlet hyper-parameter, if None set it to 1/K\n",
    "        beta: Dirichlet hyper-parameter, if None set it to 1/K\n",
    "        docs: document in the format of lists of integers: [[1,2,3,4,5,1,2], [2,4,5,1]] is two documents with 7 and 4 words respectively. \n",
    "        V: vocabulary length\n",
    "        The goal of the function is to initialize the internal variables of the class that are those provided and the counter variables described in the pseudocode.\n",
    "        \"\"\"\n",
    "\n",
    "        self.K = K\n",
    "        self.alpha = alpha # parameter of topics prior\n",
    "        self.beta = beta   # parameter of words prior\n",
    "        self.docs = docs # a list of lists, each inner list contains the indexes of the words in a doc, e.g.: [[1,2,3],[2,3,5,8,7],[1, 5, 9, 10 ,2, 5]]\n",
    "        self.V = V # how many different words in the vocabulary i.e., the number of the features of the corpus\n",
    "        # Definition of the counters \n",
    "        self.z_m_n = [] # topic assignements for each of the N words in the corpus. N: total number od words in the corpus (not the vocabulary size).\n",
    "        self.n_m_z = np.zeros((len(self.docs), K)) + alpha     # |docs|xK topics: number of words assigned to topic z in document m  \n",
    "        self.n_z_t = np.zeros((K, V)) + beta # (K topics) x |V| : number of times a word v is assigned to a topic z \n",
    "        self.n_z = np.zeros(K) + V * beta    # (K,) : overal number of words assigned to a topic z\n",
    "\n",
    "        self.N = 0\n",
    "        for m, doc in enumerate(docs):         # Initialization of the data structures I need.\n",
    "            self.N += len(doc)                 # to find the size of the corpus \n",
    "            z_n = []\n",
    "            for t in doc:\n",
    "                z = np.random.randint(0, K) # Randomly assign a topic to a word. Recall, topics have ids 0 ... K-1. randint: returns integers to [0,K[\n",
    "                z_n.append(z)                  # Keep track of the topic assigned \n",
    "                self.n_m_z[m, z] += 1          # increase the number of words assigned to topic z in the m doc.\n",
    "                self.n_z_t[z, t] += 1          #  .... number of times a word is assigned to this particular topic\n",
    "                self.n_z[z] += 1               # increase the counter of words assigned to z topic\n",
    "            self.z_m_n.append(np.array(z_n))# update the array that keeps track of the topic assignements in the words of the corpus.\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"    The learning process. Code only one iteration over the data. \n",
    "               In the main function a loop will be calling this function.     \n",
    "        \"\"\"\n",
    "        for m, doc in enumerate(self.docs):\n",
    "            z_n, n_m_z = self.z_m_n[m], self.n_m_z[m]\n",
    "            for n, t in enumerate(doc):\n",
    "                z = z_n[n]\n",
    "                n_m_z[z] -= 1\n",
    "                self.n_z_t[z, t] -= 1\n",
    "                self.n_z[z] -= 1\n",
    "                # sampling topic new_z for t\n",
    "                p_z = (self.n_z_t[:, t]+self.beta) * (n_m_z+self.alpha) / (self.n_z + self.V*self.beta)                      # A list of len() = # of topic\n",
    "                new_z = np.random.multinomial(1, p_z / p_z.sum()).argmax()   # One multinomial draw, for a distribution over topics, with probabilities summing to 1, return the index of the topic selected.\n",
    "                # set z the new topic and increment counters\n",
    "                z_n[n] = new_z\n",
    "                n_m_z[new_z] += 1\n",
    "                self.n_z_t[new_z, t] += 1\n",
    "                self.n_z[new_z] += 1\n",
    "                \n",
    "    def get_topic_dist(self):\n",
    "        \"\"\"Returns the per document topic distribution for each of the documents. \n",
    "        This is a matrix of dimensions (D documents topics) x (K topics)\n",
    "        \"\"\"\n",
    "        return self.n_m_z / self.n_m_z.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    def get_word_distributions(self):\n",
    "        \"\"\"Returns the topic-word distribution.\n",
    "        This is a matrix of  (K topics) x (V words)  \"\"\"\n",
    "        return self.n_z_t / self.n_z[:, np.newaxis]  #Normalize each line (lines are topics), with the number of words assigned to this topics to obtain probs.  *neaxis: Create an array of len = 1\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = lda_gibbs_sampling( K=10, alpha=0.1, beta=0.1, docs= documents_lda_format, V=len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    lda.inference()\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['side', 'new', 'south', 'street', 'west', 'two', 'line', 'north', 'located', 'station']\n",
      "['game', 'games', 'first', 'also', 'major', 'new', 'boston', 'american', 'league', 'played']\n",
      "['october', 'november', 'killed', 'john', 'born', 'states', 'united', 'american', 'mayor', 'served']\n",
      "['aircraft', 'air', 'world', 'corps', 'medal', 'war', 'squadron', 'states', 'united', 'marine']\n",
      "['also', 'candidates', 'members', 'formed', 'group', 'album', 'music', 'rock', 'released', 'band']\n",
      "['known', 'including', 'used', 'products', 'based', 'software', 'first', 'founded', 'also', 'company']\n",
      "['united', 'new', 'born', 'oregon', 'state', 'school', 'also', 'american', 'served', 'university']\n",
      "['league', 'team', 'york', 'college', 'one', 'national', 'new', 'played', 'football', 'american']\n",
      "['general', 'assembly', 'legislative', 'province', 'created', 'election', 'riding', 'electoral', 'provincial', 'district']\n",
      "['born', 'won', 'known', 'american', 'film', 'also', 'television', 'new', 'america', 'miss']\n"
     ]
    }
   ],
   "source": [
    "phi = lda.get_word_distributions()\n",
    "inverse_vocabulary = {val:key for key,val in vocabulary.iteritems()}\n",
    "top10 = np.argsort(phi,axis=1)[:, -10:]\n",
    "for i in top10:\n",
    "    print([inverse_vocabulary[x] for x in i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
