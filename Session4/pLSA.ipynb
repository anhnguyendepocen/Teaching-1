{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Latent Semantic Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Probabilisic Latent Semantic Analysos (pLSA) is an algorithm from the family of topic models. \n",
    "Its main goal is to model cooccurrence of \n",
    "information under a probabilistic\n",
    "framework in order to discover the underlying\n",
    "semantic structure of the data. In this sense, words that cooccur often have high probability of belonging to the same theme. \n",
    "\n",
    "Although the model has been applied in several domains (text mining, information retrieval, vision, etc.) we focus today in a text mining application that builds on what we have previously discussed in the \"Data Mining\" course, especially in the last session where we covered the vector space model. \n",
    "\n",
    "We will be speaking for three types of variables: \n",
    "* Documents: $d \\in D = \\{d_1, \\dots, d_N \\}$ observed variables. Let N be their number, defined by the size of our given corpus.\n",
    "* Words: $w \\in W = \\{w_1, \\dots , w_M\\}$ observed variables. Let M be the number of distinct words from the corpus.\n",
    "* Topics: $z \\in Z = \\{z_1, \\dots , z_K\\}$ latent (or hidden) variables. Their number, K, has to be specified a priori.\n",
    "\n",
    "The figure below shows the generative story of pLSA.  \n",
    "![The generative story of pLSA](./plsa.png)\n",
    "\n",
    "This figure reds like this: \n",
    "* First we select a document dn with probability $P(d)$.\n",
    "* For each word $w_i, i \\in \\{1, · · · , N_w \\}$ in the document $d_n$:\n",
    "     + Select a topic $z_i$  from a multinomial conditioned on the given document with probability P(z|dn).\n",
    "     + Select a word $w_i$  from a multinomial conditioned on the previous chosen topic with probability P(w|zi).\n",
    "\n",
    "\n",
    "## Inference\n",
    "We refer to inference as the process of calculating the unknown parameters of our model, while observing the observed ones. In our case we observe the words of the documents and we want to infer the probabilities. We need to decide a priori the number of topics that exist in the corpus. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: from raw text to counts\n",
    "\n",
    "We start by creating the document-term matrix for the corpus.\n",
    "* pre-process the text by cleaning it,\n",
    "* extracting the vocabulary\n",
    "* creating the matrix `document_term_matrix`, that has dimensions: |number_of_documents|x|vocabulary_size|\n",
    "\n",
    "You are encouraged to used the functions you wrote in the previous session, or the ones that have been made available in the Wiki. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 documents in the file.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "documents = open('../session3/wikipedia_data.txt').read().splitlines() # Load documents\n",
    "documents = documents[:50] # Take only 50 of them \n",
    "\n",
    "def get_vocabulary(text):\n",
    "    \"\"\"Calculates simple vocabulary counts. Returns:\n",
    "    word2id: dictionary where each word is associated with an id\n",
    "    word_counts: dictionary where the counts of the word occurences are saved\n",
    "    Prep-processing steps: only lower-casing is applied. TBD: add more elaborate pre-processing steps.\n",
    "    \"\"\"\n",
    "    word2id, word_counts, cnt={}, {}, 0\n",
    "    for document in text:\n",
    "        document = document.lower()\n",
    "        words = document.split()\n",
    "        for word in words:\n",
    "            try:\n",
    "                word2id[word]\n",
    "                word_counts[word] += 1\n",
    "            except:\n",
    "                word2id[word] = cnt\n",
    "                cnt +=1\n",
    "                word_counts[word] = 1\n",
    "    return word2id, word_counts\n",
    "\n",
    "stopwords = set( open('../session3/stop_words.txt').read().splitlines() )\n",
    "\n",
    "def clean_text(text, stop_words=None, white_space_punctuation=True, remove_non_letters=True, lower=True):\n",
    "    \"\"\"\n",
    "    Cleans the text of a document.\n",
    "    Input text: type string: the contents of a document\n",
    "    Returns: a string, with words separated by white space\n",
    "    Optional:\n",
    "        stop_words: if True, filter stop words\n",
    "        white_space_punctuation: append before and after punctuation white space\n",
    "        remove_non_letters: remove every word that does not consist of characters\n",
    "        lower: if True, lowercase the text\n",
    "    \"\"\"\n",
    "\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    if white_space_punctuation:\n",
    "        for punct_symbol in punctuation:\n",
    "            try:\n",
    "                text = text.replace(punct_symbol, \" %s \"%punct_symbol)\n",
    "            except:\n",
    "                pass\n",
    "    if remove_non_letters:\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    if stop_words != None:\n",
    "        text = [x for x in text.split() if x not in stop_words]\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "documents = [clean_text(instance, stop_words=stopwords) for instance in documents]\n",
    "print(\"There are %d documents in the file.\"%len(documents) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "def term_frequency_vectorizer(text, vocabulary=None):\n",
    "    \"\"\"\n",
    "    Returns the tf representation of a corpus in sparse format\n",
    "    Input: text: cleaned text\n",
    "    Optional: vocabulary: the number of distinct words. If it is given, use only these terms \n",
    "    and ignore out-of-vocabulary terms. Otherwise, generate the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    # do a first pass to generate vocabulary\n",
    "    if vocabulary is None:\n",
    "        vocabulary = get_vocabulary(text)[0]\n",
    "        \n",
    "        \n",
    "    row = []\n",
    "    col = []\n",
    "    values = []\n",
    "                \n",
    "    for doc_id, instance in enumerate(text):\n",
    "        words = instance.split()\n",
    "        doc_words = {}\n",
    "        for word in words:\n",
    "            try:\n",
    "                doc_words[vocabulary[word]] +=1\n",
    "            except:\n",
    "                doc_words[vocabulary[word]] = 1\n",
    "        col.extend(doc_words.keys())\n",
    "        values.extend(doc_words.values())\n",
    "        row.extend([doc_id]*len(doc_words))\n",
    "    print len(row), len(col), len(values)\n",
    "    return vocabulary, sparse.csr_matrix( (values, (row, col)), shape=(len(text), len(vocabulary))).todense()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3331 3331 3331\n"
     ]
    }
   ],
   "source": [
    "vocabulary, document_term_matrix = term_frequency_vectorizer(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point you have constructed the `document_term_matrix`.\n",
    "\n",
    "We now need to move forward to with the counter matrices we use to store the probabilities.\n",
    "We need to initialize and populate: \n",
    "* `document_topic_prob`: shape: |documents| x |topics|\n",
    "* `topic_word_probability`: shape |topics| x |vocabulary|\n",
    "* `topic_prob`: shape |documents| x |vocabulary| x |topics|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "number_of_documents = len(documents)\n",
    "number_of_topics = 10\n",
    "vocabulary_size = term_document_matrix.shape[1]\n",
    "topic_prob = np.zeros([number_of_documents, term_document_matrix.shape[1], number_of_topics], dtype=np.float) # P(d |w,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(vector):\n",
    "    sum_of_elements = sum(vector)\n",
    "    if not np.all(vector >= 0):\n",
    "        print vector\n",
    "    return vector / sum_of_elements \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_topic_prob = np.random.random(size = (number_of_documents, number_of_topics)) # P(z | d)\n",
    "topic_word_prob = np.random.random(size = (number_of_topics, term_document_matrix.shape[1] )) # P(w | z)\n",
    "\n",
    "\n",
    "for d_index in range(len(documents)):\n",
    "    document_topic_prob[d_index] = normalize(document_topic_prob[d_index]) # normalize for each document\n",
    "\n",
    "for z in range(number_of_topics):\n",
    "    topic_word_prob[z] =  normalize(topic_word_prob[z]) # normalize for each topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-step \n",
    "For the E-Step of the process one needs to calculate the following probability: \n",
    "\n",
    "* $p(z_k | d_i, w_j) = \\frac{p(w_j|z_k) p(z_k|d_i)}{\\sum\\limits_{l=1}^K p(w_j|z_k)  p(z_k|d_i)}$\n",
    "\n",
    "This can be calculated by updating the 3-d matrix while iterating over the documents and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E step: Done!\n"
     ]
    }
   ],
   "source": [
    "print \"E step:\",\n",
    "for docId, document in enumerate(documents):\n",
    "    for w_index in range(vocabulary_size):\n",
    "        prob = document_topic_prob[docId, :] * topic_word_prob[:, w_index]\n",
    "        prob = normalize(prob)\n",
    "        topic_prob[docId, w_index] = prob\n",
    "print \"Done!\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-step\n",
    "\n",
    "The step involves two updates: \n",
    "* $p(w_j| z_k) = \\frac{\\sum\\limits_{i=1}^{N} n(d_i,w_j) p(z_k|d_i,w_j)}{\\sum\\limits_{m=1}^{M} \\sum\\limits_{n=1}^{N} n(d_n, w_m) p(z_k, |d_n, w_m)}$\n",
    "* $ p(z_k|d_i) = \\frac{\\sum\\limits_{j=1}^{M} n(d_i, w_j)p(z_k|d_i, w_j) }{n(d_i)}   $\n",
    "\n",
    "\n",
    "To perform the first update `topic_word_prob` you need to iterate through topics, words and documents:\n",
    "\n",
    "\n",
    "`for each topic:\n",
    "    for each word:\n",
    "        for each document:\n",
    "            count how many times the word appears in the doc\n",
    "            s += count * topic_prob[doc, word, topic]\n",
    "        topic_word_prob[topic, word] = s\n",
    "    normalize topic_word_prob[z]`\n",
    "    \n",
    "Similarly for updating p(z|d):\n",
    "\n",
    "`for each document:\n",
    "    for each topic:\n",
    "        s = 0\n",
    "        for each word in the vocabulary:\n",
    "            count how many times the word appears in the doc\n",
    "            s += count * topic_prob[doc, word, topic]\n",
    "        document_topic_prob[doc, topic] = s\n",
    "    normalize document_topic_prob[d_index]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M step: Done!\n"
     ]
    }
   ],
   "source": [
    "# update p(w|z)\n",
    "print \"M step:\",\n",
    "for z in range(number_of_topics):\n",
    "    for w_index in range(vocabulary_size):\n",
    "        s = 0\n",
    "        for d_index in range(number_of_documents):\n",
    "            count = term_document_matrix[d_index, w_index]\n",
    "            s = s + count * topic_prob[d_index, w_index, z]\n",
    "        topic_word_prob[z, w_index] = s\n",
    "    topic_word_prob[z] = normalize(topic_word_prob[z])\n",
    "\n",
    "# update p(z|d)\n",
    "for d_index in range(len(documents)):\n",
    "    for z in range(number_of_topics):\n",
    "        s = 0\n",
    "        for w_index in range(vocabulary_size):\n",
    "            count = term_document_matrix[d_index, w_index]\n",
    "            s = s + count * topic_prob[d_index, w_index, z]\n",
    "        document_topic_prob[d_index][z] = s\n",
    "    document_topic_prob[d_index] = normalize(document_topic_prob[d_index])\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the E-step, the M-step you can iterate over these two and visualize the words with the highest probability for each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['group', 'may', 'u', 'released', 'following', 'known', 'early', 'also', 'first', 'band', 'found', 'march', 'rock', 'company', 'series', 'album', 'new', 'fillmore', 'american', 'trombone']\n"
     ]
    }
   ],
   "source": [
    "# Visualize the top words for a topic..\n",
    "inverse_vocabulary = {key:val for val,key in vocabulary.iteritems()}\n",
    "indexes = np.argsort(topic_word_prob[4])\n",
    "print [ inverse_vocabulary[i] for i in indexes[-20:] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
